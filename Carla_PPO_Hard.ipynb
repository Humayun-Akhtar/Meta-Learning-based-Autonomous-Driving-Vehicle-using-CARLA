{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import carla\n",
    "from stable_baselines3 import PPO2\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "\n",
    "from gym.utils.env_checker import check_env\n",
    "import csv\n",
    "import random\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 30\n",
    "CSV_FILE = 'waypoints.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value and policy networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class value_network(nn.Module):\n",
    "\t'''\n",
    "\tValue Network: Designed to take in state as input and give value as output\n",
    "\tUsed as a baseline in Policy Gradient (PG) algorithms\n",
    "\t'''\n",
    "\tdef __init__(self,state_dim):\n",
    "\t\t'''\n",
    "\t\t\tstate_dim (int): state dimenssion\n",
    "\t\t'''\n",
    "\t\tsuper(value_network, self).__init__()\n",
    "\t\tself.l1 = nn.Linear(state_dim, 128)\n",
    "\t\tself.l2 = nn.Linear(128, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 256)\n",
    "\t\tself.l4 = nn.Linear(256, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 128)\n",
    "\t\tself.l6 = nn.Linear(128, 64)\n",
    "\t\tself.l7 = nn.Linear(64,1)\n",
    "\n",
    "\tdef forward(self,state):\n",
    "\t\t'''\n",
    "\t\tInput: State\n",
    "\t\tOutput: Value of state\n",
    "\t\t'''\n",
    "\t\tv = F.tanh(self.l1(state))\n",
    "\t\tv = F.tanh(self.l2(v))\n",
    "\t\tv = F.tanh(self.l3(v)) \n",
    "\t\tv = F.tanh(self.l4(v)) \n",
    "\t\tv = F.tanh(self.l5(v)) \n",
    "\t\tv = F.tanh(self.l6(v)) \n",
    "\t\treturn self.l7(v)\n",
    "\t\n",
    "class policy_network(nn.Module):\n",
    "\t'''\n",
    "\tPolicy Network: Designed for continous action space, where given a \n",
    "\tstate, the network outputs the mean and standard deviation of the action\n",
    "\t'''\n",
    "\tdef __init__(self, state_dim, action_dim, log_std = 0.0):\n",
    "\t\t\"\"\" \n",
    "\t\t\tstate_dim (int): state dimenssion\n",
    "\t\t\taction_dim (int): action dimenssion\n",
    "\t\t\tlog_std (float): log of standard deviation (std)\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(policy_network, self).__init__()\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.l1 = nn.Linear(state_dim,64)\n",
    "\t\tself.l2 = nn.Linear(64,128)\n",
    "\t\tself.l3 = nn.Linear(128,256)\n",
    "\t\tself.l4 = nn.Linear(256, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 128)\n",
    "\t\tself.l7 = nn.Linear(128, 64)\n",
    "\t\tself.mean = nn.Linear(64,action_dim)\n",
    "\t\tself.log_std = nn.Parameter(torch.ones(1, action_dim) * log_std)\n",
    "\t\t\n",
    "\t\n",
    "\tdef forward(self,state):\n",
    "\t\t'''\n",
    "\t\tInput: State\n",
    "\t\tOutput: Mean, log_std and std of action\n",
    "\t\t'''\n",
    "\t\ta = F.tanh(self.l1(state))\n",
    "\t\ta = F.tanh(self.l2(a))\n",
    "\t\ta = F.tanh(self.l3(a)) \n",
    "\t\ta = F.tanh(self.l4(a)) \n",
    "\t\ta = F.tanh(self.l5(a)) \n",
    "\t\ta = F.tanh(self.l6(a))\n",
    "\t\ta = F.tanh(self.l7(a))\n",
    "\t\ta_mean = self.mean(a)\n",
    "\t\t\n",
    "\t\ta_log_std = self.log_std.expand_as(a_mean)\n",
    "\t\ta_std = torch.exp(a_log_std)\t\t\n",
    "\t\treturn a_mean, a_log_std, a_std\n",
    "\t\n",
    "\tdef select_action(self, state):\n",
    "\t\t'''\n",
    "\t\tInput: State\n",
    "\t\tOutput: Sample drawn from a normal disribution with mean and std\n",
    "\t\t'''\t\t\n",
    "\t\ta_mean, _, a_std = self.forward(state)\n",
    "\t\taction = torch.normal(a_mean)\n",
    "\t\t\n",
    "\t\treturn action\n",
    "\t\n",
    "\tdef get_log_prob(self, state, action):\n",
    "\t\t'''\n",
    "\t\tInput: State, Action\n",
    "\t\tOutput: log probabilities\n",
    "\t\t'''\n",
    "\t\tmean, log_std, std = self.forward(state)\n",
    "\t\tvar = std.pow(2)\n",
    "\t\tlog_density = -(action - mean).pow(2) / (2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n",
    "\t\treturn log_density.sum(1, keepdim=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Policy Gradient Agent - using GTBaseline update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "\t'''\n",
    "\tAn agent that performs different variants of the PG algorithm\n",
    "\t'''\n",
    "\tdef __init__(self,\n",
    "\t env,\n",
    "\t batch_size,\n",
    "\t discount=0.99,\n",
    "\t lr=1e-3,\n",
    "\t gpu_index=1,\n",
    "\t seed=0,\n",
    "\t ):\n",
    "\t\tself.env = env\n",
    "\t\tself.state_dim = 5\n",
    "\t\tself.action_dim = 2\n",
    "\t\tself.discount = discount\n",
    "\t\tself.lr = lr\n",
    "\t\tself.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\t\tself.seed = seed\n",
    "\t\tself.policy = policy_network(self.state_dim,self.action_dim)\n",
    "\t\tself.value = value_network(self.state_dim)\n",
    "\t\tself.optimizer_policy = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
    "\t\tself.optimizer_value = torch.optim.Adam(self.value.parameters(), lr=self.lr)\n",
    "\n",
    "\tdef sample_traj(self, batch_size, evaluate = False):\n",
    "\t\tself.policy.to(\"cpu\") #Move network to CPU for sampling\n",
    "\t\tstates = []\n",
    "\t\tactions = []\n",
    "\t\trewards = []\n",
    "\t\tn_dones = []\n",
    "\t\tcurr_reward_list = []\n",
    "\t\twhile len(states) < batch_size:\n",
    "\t\t\tstate, _ = self.env.reset()\n",
    "\t\t\tcurr_reward = 0\n",
    "\t\t\t#print(curr_reward)\n",
    "\t\t\tfor t in range(1000):\n",
    "\t\t\t\tstate_ten = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tif evaluate:\n",
    "\t\t\t\t\t\taction = self.policy(state_ten)[0][0].numpy() # Take mean action during evaluation\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\taction = self.policy.select_action(state_ten)[0].numpy() # Sample from distribution during training\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\taction = action.astype(np.float64)\n",
    "\t\t\t\tn_state,reward,done,_ = self.env.step(action) # Execute action in the environment\n",
    "\t\t\t\tstates.append(state)\n",
    "\t\t\t\t#print(len(states))\n",
    "\t\t\t\tactions.append(action)\n",
    "\t\t\t\trewards.append(reward)\n",
    "\t\t\t\tn_done = 0 if done else 1\n",
    "\t\t\t\tn_dones.append(n_done)\n",
    "\t\t\t\tstate = n_state\n",
    "\t\t\t\tcurr_reward += reward\n",
    "\t\t\t\tif done:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tcurr_reward_list.append(curr_reward)\n",
    "\t\tif evaluate:\n",
    "\t\t\treturn np.mean(curr_reward_list)\n",
    "\t\treturn states,actions,rewards,n_dones, np.mean(curr_reward_list)\n",
    "\t\n",
    "\tdef update(self,states,actions,rewards,n_dones):\n",
    "\n",
    "\t\tself.policy.to(self.device) #Move policy to GPU\n",
    "\n",
    "\t\tstates_ten = torch.from_numpy(np.stack(states)).to(self.device)   #Convert to tensor and move to GPU\n",
    "\t\taction_ten = torch.from_numpy(np.stack(actions)).to(self.device)  #Convert to tensor and move to GPU\n",
    "\t\trewards_ten = torch.from_numpy(np.stack(rewards)).to(self.device) #Convert to tensor and move to GPU\n",
    "\t\tn_dones_ten = torch.from_numpy(np.stack(n_dones)).to(self.device) #Convert to tensor and move to GPU\n",
    "\n",
    "\t\tstates_np = np.array(states, dtype=np.float32)\n",
    "\t\tstates_ten = torch.from_numpy(states_np).to(self.device)\n",
    "\t\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t\tself.value.to(self.device)\n",
    "\t\t\t\tvalues_adv = self.value(states_ten).to(self.device)\n",
    "\t\tgt = torch.zeros(rewards_ten.shape[0],1).to(self.device)\n",
    "\t\tg = 0\n",
    "\t\t# Compute reward-to-go (gt) and advantages\n",
    "\t\tadvantages = torch.zeros((rewards_ten.shape[0], 1)).to(self.device)\n",
    "\t\tfor i in reversed(range(rewards_ten.size(0))):\n",
    "\t\t\tg = rewards_ten[i] + self.discount * g * n_dones_ten[i]\n",
    "\t\t\tgt[i] = g\n",
    "\n",
    "\t\tadvantages = gt - values_adv\n",
    "\n",
    "\t\t# Normalize advantages\n",
    "\t\tadvantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\t\t\n",
    "\t\t# Update value network to predict gt for each state (L2 norm)\n",
    "\t\tloss = torch.nn.MSELoss()\n",
    "\t\tvalue_loss = loss(self.value(states_ten), gt)\n",
    "\t\tself.optimizer_value.zero_grad()\n",
    "\t\t#with torch.no_grad():\n",
    "\t\tvalue_loss.backward()\n",
    "\t\tself.optimizer_value.step()\n",
    "\n",
    "\t\t# Compute log probabilities using states_ten and action_ten\n",
    "\t\tlog_probs = self.policy.get_log_prob(states_ten, action_ten)\n",
    "\n",
    "\t\t# Compute policy loss (using advantages) and update the policy\n",
    "\t\tself.optimizer_policy.zero_grad()\n",
    "\t\tpolicy_loss = (-log_probs * advantages.detach()).mean()\n",
    "\t\tpolicy_loss.backward()\n",
    "\t\tself.optimizer_policy.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_waypoints(file_path):\n",
    "    waypoints = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            x, y, yaw = map(float, row)\n",
    "            waypoints.append((x, y, yaw))\n",
    "            \n",
    "    return waypoints\n",
    "\n",
    "\n",
    "def setupCarla():\n",
    "\n",
    "    # Connect to CARLA server\n",
    "    client = carla.Client('localhost', 2000)\n",
    "    client.set_timeout(10.0)\n",
    "\n",
    "    return client.get_world()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CARLA wrapper based on gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarlaInstance(gym.Env):\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, world, waypoints):\n",
    "        super(CarlaInstance, self).__init__()\n",
    "        try:\n",
    "            self.waypoints = waypoints\n",
    "            self.current_waypoint_index = 0\n",
    "\n",
    "            self.blueprint_library = world.get_blueprint_library()\n",
    "            world.set_weather(carla.WeatherParameters.ClearNoon)\n",
    "\n",
    "\n",
    "            # Spawn vehicle\n",
    "            vehicle_bp = random.choice(self.blueprint_library.filter('wrangler_rubicon'))\n",
    "            spawn_point = carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0))\n",
    "            self.vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "            #self.vehicle.set_simulate_physics(True)\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer=0.0))\n",
    "            sleep(2.0)\n",
    "\n",
    "            # Attach Lane Invasion Sensor to car\n",
    "            sensor_bp = world.get_blueprint_library().find('sensor.other.lane_invasion')\n",
    "            sensor_transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "            self.lane_invasion_sensor = world.spawn_actor(sensor_bp, sensor_transform, attach_to=self.vehicle)\n",
    "            self.lane_invasion_sensor.listen(lambda event: self.on_lane_invasion(event))\n",
    "\n",
    "            self.collision_sensor = world.spawn_actor(world.get_blueprint_library().find('sensor.other.collision'), sensor_transform, attach_to=self.vehicle)\n",
    "            self.collision_sensor.listen(lambda event: self.on_lane_invasion(event))\n",
    "\n",
    "            self.action_space = spaces.Tuple((spaces.Box(low=0.0, high=1.0, dtype=np.float64, shape=(1,)),\n",
    "                                              spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float64)))\n",
    "            self.observation_space = spaces.Box(low=-500000, high=500000, shape=(5,), dtype=np.float64)\n",
    "            self.problem = bool\n",
    "\n",
    "        except RuntimeError or KeyboardInterrupt:\n",
    "            print(\"entered init exception\")\n",
    "            for actor in world.get_actors():\n",
    "                actor.destroy()\n",
    "            pass\n",
    "\n",
    "    def on_lane_invasion(self, event):\n",
    "        self.problem = True \n",
    "        #print(\"Theek kar\")\n",
    "        self.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        #print(\"{}\\n\", format(action))\n",
    "        \n",
    "        throttle = action[0]\n",
    "        steer = action[1]\n",
    "        brake = 0\n",
    "        # print(action)\n",
    "        if action.any() == None:\n",
    "            reward = -10000\n",
    "\n",
    "        # if action.all() == 0:\n",
    "        #     steer = 0\n",
    "        # elif action.all() == 1:\n",
    "        #     steer = 1.0                                \n",
    "        # elif action.all() == 2:\n",
    "        #     steer = -1.0\n",
    "        # elif action.all() == 3:\n",
    "        #     throttle = 0\n",
    "        # elif action.all() == 4:\n",
    "        #     throttle = 1\n",
    "\n",
    "        control = carla.VehicleControl(throttle=throttle, brake=brake, steer=steer)\n",
    "        self.vehicle.apply_control(control)\n",
    "\n",
    "        next_waypoint = self.waypoints[self.current_waypoint_index]\n",
    "        next_waypoint_location = carla.Location(x=next_waypoint[0], y=next_waypoint[1])\n",
    "        current_location = self.vehicle.get_location()\n",
    "        distance = current_location.distance(next_waypoint_location)\n",
    "\n",
    "        if distance < threshold:\n",
    "            self.current_waypoint_index += 1\n",
    "            if self.current_waypoint_index >= len(self.waypoints):\n",
    "                self.current_waypoint_index = 0\n",
    "\n",
    "        reward = self.get_reward(distance, control)\n",
    "        done = False\n",
    "        if self.current_waypoint_index == len(self.waypoints) - 1 or self.problem:\n",
    "            done = True\n",
    "#         print (f'step taken: Throttle: {throttle} and steer: {steer}')\n",
    "        info = {}\n",
    "        sleep(0.05)\n",
    "\n",
    "        return self.get_observation(), reward, done, info\n",
    "    \n",
    "    \n",
    "    def get_observation(self):\n",
    "        location = self.vehicle.get_location()\n",
    "        orientation = self.vehicle.get_transform().rotation.yaw\n",
    "        speed = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(speed.x**2 + speed.y**2 + speed.z**2)\n",
    "        next_waypoint = self.waypoints[self.current_waypoint_index]\n",
    "        next_waypoint_location = carla.Location(x=next_waypoint[0], y=next_waypoint[1])\n",
    "        distance = location.distance(next_waypoint_location)\n",
    "#         print('observation taken')\n",
    "\n",
    "        return np.array([location.x, location.y, orientation, speed, distance], dtype=np.float64)\n",
    "    \n",
    "    def get_reward(self, distance, control):\n",
    "        reward = 0\n",
    "        if control.throttle > 0:\n",
    "            reward += 5\n",
    "        if distance < threshold:\n",
    "            reward += 100    \n",
    "        # Apply a large negative reward for lane invasion\n",
    "        if self.problem:\n",
    "            reward -= 1000  # Adjust the value as needed\n",
    "#         print('reward calculated')    \n",
    "        return reward\n",
    "    \n",
    "    def close(self):\n",
    "        self.vehicle.destroy()\n",
    "        self.lane_invasion_sensor.destroy()\n",
    "        self.collision_sensor.destroy()    \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        vel = carla.Vector3D()\n",
    "        vel.x = 0\n",
    "        vel.y = 0\n",
    "        vel.z = 0\n",
    "        self.vehicle.set_target_velocity(vel)\n",
    "        x, y, yaw = self.waypoints[0]\n",
    "        transform = carla.Transform(carla.Location(x=-23.6,y=137.5),carla.Rotation(yaw=0))\n",
    "        self.vehicle.set_transform(transform)\n",
    "        self.current_waypoint_index = 0        \n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.get_observation(), info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    seed=0          # Sets Gym, PyTorch and Numpy seeds\n",
    "    n_iter = 900      # Maximum number of training iterations\n",
    "    discount=0.99   # Discount factor\n",
    "    batch_size=100 # Training samples in each batch of training\n",
    "    lr=8e-3       # Learning rate\n",
    "    gpu_index=1\t\t# GPU index\n",
    "\n",
    "    waypoints = read_waypoints(CSV_FILE)\n",
    "    world = setupCarla()\n",
    "    \n",
    "    #Creating and verifying the carla environment instance\n",
    "    env = CarlaInstance(world, waypoints)\n",
    "    #check_env(env)\n",
    "\n",
    "    # Setting seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    kwargs = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"discount\":discount,\n",
    "        \"lr\":lr,\n",
    "        \"gpu_index\":gpu_index,\n",
    "        \"seed\":seed,\n",
    "        \"env\":env\n",
    "    }\t\n",
    "    learner = PGAgent(**kwargs) # Creating the PG learning agent\n",
    "    average_rewards=[]\n",
    "    moving_window = deque(maxlen=10)\n",
    "    old_reward=-1\n",
    "    for e in range(n_iter):\n",
    "        states,actions,rewards,n_dones,train_reward = learner.sample_traj(batch_size=batch_size)\n",
    "        learner.update(states,actions,rewards,n_dones)\n",
    "        eval_reward= learner.sample_traj(batch_size, evaluate=True)\n",
    "        moving_window.append(eval_reward)\n",
    "        if not e: print('Training Iteration {} Training Reward: {:.2f} Evaluation Reward: {:.2f} \\\n",
    "        Average Evaluation Reward: {:.2f}'.format(e,train_reward,eval_reward,np.mean(moving_window)))\n",
    "        \n",
    "        average_rewards.append(np.mean(moving_window))\n",
    "\n",
    "        if np.mean(moving_window) > old_reward:\n",
    "            old_reward = np.mean(moving_window)\n",
    "            torch.save(learner.policy.state_dict(), ('CARLA_PPO_checkpoint1.pth'))\n",
    "\n",
    "    window_size = 20\n",
    "    averages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
