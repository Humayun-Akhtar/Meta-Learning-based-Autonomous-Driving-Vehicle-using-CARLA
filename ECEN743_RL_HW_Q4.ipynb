{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bLXT2Qv8NPF2",
        "outputId": "569b8553-1aab-4be9-b402-0f13d027bf83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting pygame==2.1.3\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, farama-notifications, box2d-py, pygame, jax-jumpy, gymnasium\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 pygame-2.1.3 swig-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dW_rsWvPbD8q"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LE7WesRNbD8s"
      },
      "outputs": [],
      "source": [
        "class value_network(nn.Module):\n",
        "\t'''\n",
        "\tValue Network: Designed to take in state as input and give value as output\n",
        "\tUsed as a baseline in Policy Gradient (PG) algorithms\n",
        "\t'''\n",
        "\tdef __init__(self,state_dim):\n",
        "\t\t'''\n",
        "\t\t\tstate_dim (int): state dimenssion\n",
        "\t\t'''\n",
        "\t\tsuper(value_network, self).__init__()\n",
        "\t\tself.l1 = nn.Linear(state_dim, 128)\n",
        "\t\tself.l2 = nn.Linear(128, 64)\n",
        "\t\tself.l3 = nn.Linear(64, 1)\n",
        "\n",
        "\tdef forward(self,state):\n",
        "\t\t'''\n",
        "\t\tInput: State\n",
        "\t\tOutput: Value of state\n",
        "\t\t'''\n",
        "\t\tv = F.tanh(self.l1(state))\n",
        "\t\tv = F.tanh(self.l2(v))\n",
        "\t\treturn self.l3(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V3UhmR72bD8t"
      },
      "outputs": [],
      "source": [
        "class policy_network(nn.Module):\n",
        "\t'''\n",
        "\tPolicy Network: Designed for continous action space, where given a \n",
        "\tstate, the network outputs the mean and standard deviation of the action\n",
        "\t'''\n",
        "\tdef __init__(self,state_dim,action_dim,log_std = 0.0):\n",
        "\t\t\"\"\"\n",
        "\t\t\tstate_dim (int): state dimenssion\n",
        "\t\t\taction_dim (int): action dimenssion\n",
        "\t\t\tlog_std (float): log of standard deviation (std)\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(policy_network, self).__init__()\n",
        "\t\tself.state_dim = state_dim\n",
        "\t\tself.action_dim = action_dim\n",
        "\t\tself.l1 = nn.Linear(state_dim,64)\n",
        "\t\tself.l2 = nn.Linear(64,64)\n",
        "\t\t#self.l3 = nn.Linear(64,64)\n",
        "\t\t#self.l4 = nn.Linear(64,64)\n",
        "\t\tself.mean = nn.Linear(64,action_dim)\n",
        "\t\tself.log_std = nn.Parameter(torch.ones(1, action_dim) * log_std)\n",
        "\t\t\n",
        "\t\n",
        "\tdef forward(self,state):\n",
        "\t\t'''\n",
        "\t\tInput: State\n",
        "\t\tOutput: Mean, log_std and std of action\n",
        "\t\t'''\n",
        "\t\ta = F.tanh(self.l1(state))\n",
        "\t\ta = F.tanh(self.l2(a))\n",
        "\t\ta_mean = self.mean(a)\n",
        "\t\ta_log_std = self.log_std.expand_as(a_mean)\n",
        "\t\ta_std = torch.exp(a_log_std)\t\t\n",
        "\t\treturn a_mean, a_log_std, a_std\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\t'''\n",
        "\t\tInput: State\n",
        "\t\tOutput: Sample drawn from a normal disribution with mean and std\n",
        "\t\t'''\n",
        "\n",
        "\t\t\n",
        "\t\ta_mean, _, a_std = self.forward(state)\n",
        "\t\taction = torch.normal(a_mean)\n",
        "\t\treturn action\n",
        "\t\n",
        "\tdef get_log_prob(self, state, action):\n",
        "\t\t'''\n",
        "\t\tInput: State, Action\n",
        "\t\tOutput: log probabilities\n",
        "\t\t'''\n",
        "\t\tmean, log_std, std = self.forward(state)\n",
        "\t\tvar = std.pow(2)\n",
        "\t\tlog_density = -(action - mean).pow(2) / (2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n",
        "\t\treturn log_density.sum(1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWNMolJwbD8u"
      },
      "outputs": [],
      "source": [
        "class PGAgent():\n",
        "\t'''\n",
        "\tAn agent that performs different variants of the PG algorithm\n",
        "\t'''\n",
        "\tdef __init__(self,\n",
        "\t state_dim, \n",
        "\t action_dim,\n",
        "\t discount=0.99,\n",
        "\t lr=1e-3,\n",
        "\t gpu_index=0,\n",
        "\t seed=0,\n",
        "\t env=\"LunarLander-v2\"\n",
        "\t ):\n",
        "\t\t\"\"\"\n",
        "\t\t\tstate_size (int): dimension of each state\n",
        "\t\t\taction_size (int): dimension of each action\n",
        "\t\t\tdiscount (float): discount factor\n",
        "\t\t\tlr (float): learning rate\n",
        "\t\t\tgpu_index (int): GPU used for training\n",
        "\t\t\tseed (int): Seed of simulation\n",
        "\t\t\tenv (str): Name of environment\n",
        "\t\t\"\"\"\n",
        "\t\tself.state_dim = state_dim\n",
        "\t\tself.action_dim = action_dim\n",
        "\t\tself.discount = discount\n",
        "\t\tself.lr = lr\n",
        "\t\tself.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
        "\t\tself.env_name = env\n",
        "\t\tself.seed = seed\n",
        "\t\tself.policy = policy_network(state_dim,action_dim)\n",
        "\t\tself.value = value_network(state_dim)\n",
        "\t\tself.optimizer_policy = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\t\tself.optimizer_value = torch.optim.Adam(self.value.parameters(), lr=self.lr)\n",
        "\n",
        "\tdef sample_traj(self,batch_size=4000,evaluate = False):\n",
        "\t\t'''\n",
        "\t\tInput: \n",
        "\t\t\tbatch_size: minimum batch size needed for update\n",
        "\t\t\tevaluate: flag to be set during evaluation\n",
        "\t\tOutput:\n",
        "\t\t\tstates, actions, rewards,not_dones, episodic reward \t\n",
        "\t\t'''\n",
        "\t\tself.policy.to(\"cpu\") #Move network to CPU for sampling\n",
        "\t\tenv = gym.make(self.env_name)\n",
        "\t\tstates = []\n",
        "\t\tactions = []\n",
        "\t\trewards = []\n",
        "\t\tn_dones = []\n",
        "\t\tcurr_reward_list = []\n",
        "\t\twhile len(states) < batch_size:\n",
        "\t\t\tstate, _ = env.reset(seed=self.seed)\n",
        "\t\t\tcurr_reward = 0\n",
        "\t\t\tfor t in range(1500):\n",
        "\t\t\t\tstate_ten = torch.from_numpy(state).float().unsqueeze(0)\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tif evaluate:\n",
        "\t\t\t\t\t\taction = self.policy(state_ten)[0][0].numpy() # Take mean action during evaluation\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\taction = self.policy.select_action(state_ten)[0].numpy() # Sample from distribution during training\n",
        "\t\t\t\taction = action.astype(np.float32)\n",
        "\t\t\t\tn_state,reward,terminated,truncated,_ = env.step(action) # Execute action in the environment\n",
        "\t\t\t\tdone = terminated or truncated\n",
        "\t\t\t\tstates.append(state)\n",
        "\t\t\t\tactions.append(action)\n",
        "\t\t\t\trewards.append(reward)\n",
        "\t\t\t\tn_done = 0 if done else 1\n",
        "\t\t\t\tn_dones.append(n_done)\n",
        "\t\t\t\tstate = n_state\n",
        "\t\t\t\tcurr_reward += reward\n",
        "\t\t\t\tif done:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\tcurr_reward_list.append(curr_reward)\n",
        "\t\tif evaluate:\n",
        "\t\t\treturn np.mean(curr_reward_list)\n",
        "\t\treturn states,actions,rewards,n_dones, np.mean(curr_reward_list)\n",
        "\t\n",
        "\n",
        "\n",
        "\n",
        "\tdef update(self,states,actions,rewards,n_dones,update_type='Baseline'):\n",
        "\n",
        "\t\tself.policy.to(self.device) #Move policy to GPU\n",
        "\t\tif update_type == \"Baseline\":\n",
        "\t\t\tself.value.to(self.device)\t#Move value to GPU\n",
        "\t\tstates_ten = torch.from_numpy(np.stack(states)).to(self.device)   #Convert to tensor and move to GPU\n",
        "\t\taction_ten = torch.from_numpy(np.stack(actions)).to(self.device)  #Convert to tensor and move to GPU\n",
        "\t\trewards_ten = torch.from_numpy(np.stack(rewards)).to(self.device) #Convert to tensor and move to GPU\n",
        "\t\tn_dones_ten = torch.from_numpy(np.stack(n_dones)).to(self.device) #Convert to tensor and move to GPU\n",
        "\n",
        "\t\t\n",
        "\t\tif update_type == \"Rt\":\n",
        "\n",
        "\t\t\trt = torch.zeros(rewards_ten.shape[0],1).to(self.device)\n",
        "\t\t\trt_accum = 0\n",
        "\t\t\tfor t in reversed(range(rewards_ten.shape[0])):\n",
        "\t\t\t\t\trt_accum = rewards_ten[t] + rt_accum * self.discount *n_dones_ten[t]\n",
        "\t\t\t\t\trt[t] = rt_accum\n",
        "\n",
        "\t\t\tlog_prob = self.policy.get_log_prob(states_ten, action_ten)\n",
        "\t\t\tpolicy_loss = -(log_prob * rt.detach()).mean()\n",
        "\n",
        "\t\t\tself.optimizer_policy.zero_grad()\n",
        "\t\t\tpolicy_loss.backward()\n",
        "\t\t\tself.optimizer_policy.step()\n",
        "\n",
        "\t\tif update_type == 'Gt':\n",
        "\t\t\tgt = torch.zeros(rewards_ten.shape[0],1).to(self.device)\n",
        "\t\t\tg = 0\n",
        "\t\t\tfor i in reversed(range(rewards_ten.size(0))):\n",
        "\t\t\t\tg = rewards_ten[i] + self.discount * g *(n_dones_ten[i])\n",
        "\t\t\t\tgt[i] = g\n",
        "\n",
        "\t\t\tgt = (gt - gt.mean()) / gt.std() #Helps with learning stablity\n",
        "\t\t\tlog_prob = self.policy.get_log_prob(states_ten, action_ten)\n",
        "\t\t\tpolicy_loss = -(log_prob * gt.detach()).mean()\n",
        "\n",
        "\t\t\tself.optimizer_policy.zero_grad()\n",
        "\t\t\tpolicy_loss.backward()\n",
        "\t\t\tself.optimizer_policy.step()\n",
        "\n",
        "\t\tif update_type == 'GTBaseline':\n",
        "\t\t\tstate_t = torch.FloatTensor(states).to(self.device)\n",
        "        \n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tself.value.to(self.device)\n",
        "\t\t\t\t\tvalues_adv = self.value(states_ten).to(self.device)\n",
        "\t\t\tgt = torch.zeros(rewards_ten.shape[0],1).to(self.device)\n",
        "\t\t\tg=0\n",
        "\t\t\t# Compute reward-to-go (gt) and advantages\n",
        "\t\t\treturns = torch.zeros((rewards_ten.shape[0], 1)).to(self.device)\n",
        "\t\t\tadvantages = torch.zeros((rewards_ten.shape[0], 1)).to(self.device)\n",
        "\t\t\tfor i in reversed(range(rewards_ten.size(0))):\n",
        "\t\t\t\tg = rewards_ten[i] + self.discount * g * n_dones_ten[i]\n",
        "\t\t\t\tgt[i] = g\n",
        "\n",
        "\n",
        "\t\t\tadvantages = gt - values_adv\n",
        "\n",
        "\n",
        "\t\t\t# Normalize advantages\n",
        "\t\t\tadvantages = (advantages - advantages.mean()) / advantages.std()\n",
        "\t\t\t\n",
        "\t\t\t# Update value network to predict gt for each state (L2 norm)\n",
        "\t\t\tloss = torch.nn.MSELoss()\n",
        "\t\t\tvalue_loss = loss(self.value(states_ten), gt)\n",
        "\t\t\tself.optimizer_value.zero_grad()\n",
        "\t\t\t#with torch.no_grad():\n",
        "\t\t\tvalue_loss.backward()\n",
        "\t\t\tself.optimizer_value.step()\n",
        "\n",
        "\t\t\t# Compute log probabilities using states_ten and action_ten\n",
        "\t\t\tlog_probs = self.policy.get_log_prob(states_ten, action_ten)\n",
        "\n",
        "\t\t\t# Compute policy loss (using advantages) and update the policy\n",
        "\t\t\tself.optimizer_policy.zero_grad()\n",
        "\t\t\tpolicy_loss = (-log_probs * advantages.detach()).mean()\n",
        "\t\t\tpolicy_loss.backward()\n",
        "\t\t\tself.optimizer_policy.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rc21PEIUbD8v"
      },
      "outputs": [],
      "source": [
        "def run(algo):\n",
        "\tenv_type=\"MountainCarContinuous-v0\" # Gymnasium environment name\n",
        "\tseed=0          # Sets Gym, PyTorch and Numpy seeds\n",
        "\tn_iter = 10000      # Maximum number of training iterations\n",
        "\tdiscount=0.99   # Discount factor\n",
        "\tbatch_size=5000 # Training samples in each batch of training\n",
        "\tlr=5e-4       # Learning rate\n",
        "\tgpu_index=0\t\t# GPU index\n",
        "\talgo=algo\t\t  # PG algorithm type. Baseline_with_Gt/Gt/Rt\n",
        "\t\n",
        "\n",
        "\t# Making the environment\t\n",
        "\tenv = gym.make(env_type)\n",
        "\n",
        "\t# Setting seeds\n",
        "\ttorch.manual_seed(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\n",
        "\tstate_dim = env.observation_space.shape[0]\n",
        "\tprint(state_dim)\n",
        "\taction_dim = env.action_space.shape[0]\n",
        "\n",
        "\tkwargs = {\n",
        "\t\t\"state_dim\":state_dim,\n",
        "\t\t\"action_dim\":action_dim,\n",
        "\t\t\"discount\":discount,\n",
        "\t\t\"lr\":lr,\n",
        "\t\t\"gpu_index\":gpu_index,\n",
        "\t\t\"seed\":seed,\n",
        "\t\t\"env\":env_type\n",
        "\t}\t\n",
        "\tlearner = PGAgent(**kwargs) # Creating the PG learning agent\n",
        "\taverage_rewards=[]\n",
        "\tmoving_window = deque(maxlen=10)\n",
        "\told_reward=-1\n",
        "\tfor e in range(n_iter):\n",
        "\t\tstates,actions,rewards,n_dones,train_reward = learner.sample_traj(batch_size=batch_size)\n",
        "\t\tlearner.update(states,actions,rewards,n_dones,algo)\n",
        "\t\teval_reward= learner.sample_traj(evaluate=True)\n",
        "\t\tmoving_window.append(eval_reward)\n",
        "\t\tif not e%100: print('Training Iteration {} Training Reward: {:.2f} Evaluation Reward: {:.2f} \\\n",
        "\t\tAverage Evaluation Reward: {:.2f}'.format(e,train_reward,eval_reward,np.mean(moving_window)))\n",
        "\t\t\n",
        "\t\taverage_rewards.append(np.mean(moving_window))\n",
        "\t\n",
        "\t\tif np.mean(moving_window) > old_reward:\n",
        "\t\t\told_reward = np.mean(moving_window)\n",
        "\t\t\ttorch.save(learner.policy.state_dict(), (algo + '_checkpoint1.pth'))\n",
        "\n",
        "\n",
        "\tplt.plot(average_rewards, color='g')\n",
        "\tplt.ylabel('Episodic Cumulative Reward')\n",
        "\tplt.xlabel('Episode #')\n",
        "\tplt.title('Curve for episodic cumulative Reward for update type = {}'.format(algo))\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ec6tlzMiRQtf",
        "outputId": "952c650b-267f-4828-93da-d389a955b515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Training Iteration 0 Training Reward: -101.99 Evaluation Reward: -0.27 \t\tAverage Evaluation Reward: -0.27\n",
            "Training Iteration 100 Training Reward: -100.50 Evaluation Reward: -0.02 \t\tAverage Evaluation Reward: -0.09\n",
            "Training Iteration 200 Training Reward: -65.34 Evaluation Reward: -3.57 \t\tAverage Evaluation Reward: -2.85\n",
            "Training Iteration 300 Training Reward: -77.66 Evaluation Reward: -0.14 \t\tAverage Evaluation Reward: -0.05\n",
            "Training Iteration 400 Training Reward: -104.63 Evaluation Reward: -0.15 \t\tAverage Evaluation Reward: -1.50\n",
            "Training Iteration 500 Training Reward: -104.04 Evaluation Reward: -2.12 \t\tAverage Evaluation Reward: -1.86\n",
            "Training Iteration 600 Training Reward: -103.02 Evaluation Reward: -2.10 \t\tAverage Evaluation Reward: -1.13\n",
            "Training Iteration 700 Training Reward: -26.42 Evaluation Reward: 39.76 \t\tAverage Evaluation Reward: -3.11\n",
            "Training Iteration 800 Training Reward: -17.91 Evaluation Reward: 57.57 \t\tAverage Evaluation Reward: 53.35\n",
            "Training Iteration 900 Training Reward: 19.54 Evaluation Reward: 77.03 \t\tAverage Evaluation Reward: 73.64\n"
          ]
        }
      ],
      "source": [
        "run(\"GTBaseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_f10q5yAc19"
      },
      "outputs": [],
      "source": [
        " #For visualization\n",
        "import gymnasium as gym\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import glob\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxlzJYd3Adl1"
      },
      "outputs": [],
      "source": [
        "def show_video_of_model(agent, env_name, algo):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video = cv2.VideoWriter(algo+'_trained.mp4', fourcc, 30, (600, 400))\n",
        "    agent.policy.load_state_dict(torch.load(algo+\"_checkpoint1.pth\"))\n",
        "    agent.policy.eval()\n",
        "    state, _= env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        video.write(frame)\n",
        "        state_ten = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        action = agent.policy.select_action(state_ten)[0].detach().numpy()\n",
        "        action = action.astype(np.float64)\n",
        "        n_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated \n",
        "        state = n_state\n",
        "    env.close()\n",
        "    video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaXrD0AEAgs2"
      },
      "outputs": [],
      "source": [
        "env_type = \"MountainCarContinuous-v0\"\n",
        "env = gym.make(env_type)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "plotter_agent = PGAgent(state_dim,action_dim)\n",
        "\n",
        "show_video_of_model(plotter_agent, \"MountainCarContinuous-v0\", \"Gt\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}