{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f59e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef385cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_MAML_Network(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPO_MAML_Network, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_mean = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_mean, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1417fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarlaEnvironment:\n",
    "    def __init__(self, world, vehicle_blueprint, waypoints):\n",
    "        self.world = world\n",
    "        self.vehicle_blueprint = vehicle_blueprint\n",
    "        self.waypoints = waypoints\n",
    "        self.reset()\n",
    "        self.actorlist = []\n",
    "    def reset(self):\n",
    "        self.vehicle = self.world.spawn_actor(self.vehicle_blueprint, carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0)))\n",
    "        self.current_waypoint = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply actions: Throttle, Steering, Brake\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=action[0], steer=action[1], brake=action[2]))\n",
    "        self.actorlist.append(self.vehicle)\n",
    "\n",
    "        # Get next state\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(next_state)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.is_done(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "#     def get_state(self):\n",
    "#         curr_vel = self.vehicle.get_velocity()\n",
    "#         curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "#         waypoint = self.waypoints[self.current_waypoint]\n",
    "#         return np.array([curr_vel, curr_yaw - waypoint[1], curr_yaw - waypoint[2], waypoint[3]])\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = []\n",
    "        curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "        \n",
    "\n",
    "        for i in range(10):\n",
    "            if self.current_waypoint + i < len(self.waypoints):\n",
    "                waypoint = self.waypoints[self.current_waypoint + i]\n",
    "                curr_vel = self.vehicle.get_velocity()\n",
    "                curr_loc = self.vehicle.get_location()\n",
    "                distance = curr_loc.distance(x=waypoint[4],y=waypoint[5])\n",
    "                state.extend([curr_vel, curr_yaw - waypoint[1], curr_yaw - waypoint[2], distance ])\n",
    "            else:\n",
    "                state.extend([0, 0, 0, 0])\n",
    "\n",
    "        return np.array(state)\n",
    "\n",
    "    def generate_waypoints(self):\n",
    "        # Implement waypoint generation logic\n",
    "        pass\n",
    "\n",
    "    def calculate_reward(self, state):\n",
    "        reward = -(np.linalg.norm(state[0][0] - self.waypoints[self.current_waypoint][0]) - (np.linalg.norm (state[0][1])) - (np.linalg.norm ( state[0][2])) - (np.linalg.norm (state[0][3])) + self.current_waypoint)\n",
    "        return reward\n",
    "\n",
    "    def is_done(self, state):\n",
    "        if np.abs(state[3]) < 0.20:  # If the vehicle is close to the next waypoint\n",
    "            self.current_waypoint += 1\n",
    "\n",
    "        if self.current_waypoint >= len(self.waypoints):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def set_weather(self, weather):\n",
    "        self.world.set_weather(weather)\n",
    "        \n",
    "    def destroy(self):\n",
    "        for actor in self.actorlist:\n",
    "            actor.destroy()\n",
    "        print(\"All Cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568d18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weather_conditions(num_weather_conditions):\n",
    "    weather_types = [carla.WeatherParameters.ClearNoon, carla.WeatherParameters.WetNoon,\n",
    "                     carla.WeatherParameters.WetCloudyNoon, carla.WeatherParameters.HeavyRainNoon]\n",
    "    return np.random.choice(weather_types, num_weather_conditions)\n",
    "\n",
    "def sample_trajectories(env, model, num_episodes=10, max_episode_length=1000):\n",
    "    trajectories = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "\n",
    "        for _ in range(max_episode_length):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_mean, _ = model(state_tensor)\n",
    "            action_distribution = Normal(action_mean, torch.tensor(0.1))\n",
    "            action = action_distribution.sample().detach().numpy()\n",
    "            next_state, reward, done = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        trajectories.append(trajectory)\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "def compute_advantages_and_returns(trajectories, model, gamma=0.99, lam=0.95):\n",
    "    advantages = []\n",
    "    returns = []\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        traj_advantages = []\n",
    "        traj_returns = []\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "\n",
    "        for state, _, reward, next_state, done in reversed(trajectory):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            _, value = model(state_tensor)\n",
    "            _, next_value = model(next_state_tensor)\n",
    "\n",
    "            if done:\n",
    "                delta = reward - prev_value\n",
    "                traj_advantages.append(delta)\n",
    "                traj_returns.append(reward)\n",
    "            else:\n",
    "                delta = reward + gamma * next_value - prev_value\n",
    "                advantage = delta + gamma * lam * prev_advantage\n",
    "                traj_advantages.append(advantage)\n",
    "                traj_returns.append(reward + gamma * prev_advantage)\n",
    "\n",
    "            prev_value = value\n",
    "            prev_advantage = advantage\n",
    "\n",
    "        traj_advantages.reverse()\n",
    "        traj_returns.reverse()\n",
    "\n",
    "        advantages.extend(traj_advantages)\n",
    "        returns.extend(traj_returns)\n",
    "\n",
    "    return advantages, returns\n",
    "\n",
    "def compute_losses(trajectories, model, advantages, returns, clip_ratio=0.2, value_coeff=0.5, entropy_coeff=0.01):\n",
    "    state_tensor = torch.tensor([state for trajectory in trajectories for state, _, _, _, _ in trajectory], dtype=torch.float32)\n",
    "    action_tensor = torch.tensor([action for trajectory in trajectories for _, action, _, _, _ in trajectory], dtype=torch.float32)\n",
    "    advantage_tensor = torch.tensor(advantages, dtype=torch.float32)\n",
    "    return_tensor = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    action_mean, value = model(state_tensor)\n",
    "    action_distribution = Normal(action_mean, torch.tensor(0.1))\n",
    "    log_probs = action_distribution.log_prob(action_tensor)\n",
    "\n",
    "    old_action_mean, old_value = model(state_tensor)\n",
    "    old_action_distribution = Normal(old_action_mean, torch.tensor(0.1))\n",
    "    old_log_probs = old_action_distribution.log_prob(action_tensor).detach()\n",
    "\n",
    "    ratio = torch.exp(log_probs - old_log_probs)\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "    policy_loss = -torch.min(ratio * advantage_tensor, clipped_ratio * advantage_tensor).mean()\n",
    "\n",
    "    value_loss = value_coeff * (return_tensor - value).pow(2).mean()\n",
    "\n",
    "    entropy_loss = entropy_coeff * action_distribution.entropy().mean()\n",
    "\n",
    "    total_loss = policy_loss + value_loss - entropy_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def train_ppo_maml(env, state_dim, action_dim, lr=1e-4, n_iters=1000000, inner_loop_steps=5, num_weather_conditions=4):\n",
    "    model = PPO_MAML_Network(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        weather_conditions = sample_weather_conditions(num_weather_conditions)\n",
    "        task_gradients = []\n",
    "\n",
    "        for weather in weather_conditions:\n",
    "            env.set_weather(weather)\n",
    "            trajectories = sample_trajectories(env, model)\n",
    "            advantages, returns = compute_advantages_and_returns(trajectories, model)\n",
    "            inner_loop_gradients = []\n",
    "\n",
    "            for _ in range(inner_loop_steps):\n",
    "                total_loss = compute_losses(trajectories, model, advantages, returns)\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                inner_loop_gradients.append([p.grad.clone() for p in model.parameters()])\n",
    "\n",
    "            task_gradients.append(inner_loop_gradients)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            mean_gradient = torch.mean(torch.stack([g[i] for g in task_gradients]), dim=0)\n",
    "            param.grad = mean_gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"Iteration: \", iteration)\n",
    "            \n",
    "def read_waypoints(file_path):\n",
    "    waypoints = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            x, y, yaw, r = map(float, row)\n",
    "            waypoints.append((x, y, yaw, r))\n",
    "            \n",
    "    return waypoints                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee39e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API \n",
      "WARNING: Client API version     = 0.9.14 \n",
      "WARNING: Simulator API version  = 0.9.14-4-gf14acb257 \n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    Location.distance(Location, float)\ndid not match C++ signature:\n    distance(carla::geom::Location {lvalue}, carla::geom::Location location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m vehicle_blueprint \u001b[38;5;241m=\u001b[39m blueprint_library\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvehicle.jeep.wrangler_rubicon\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m waypoints \u001b[38;5;241m=\u001b[39m read_waypoints(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaypoint_distance_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCarlaEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle_blueprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaypoints\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Implement CarlaEnvironment with the required state and action spaces\u001b[39;00m\n\u001b[1;32m     13\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[1;32m     14\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mCarlaEnvironment.__init__\u001b[0;34m(self, world, vehicle_blueprint, waypoints)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle_blueprint \u001b[38;5;241m=\u001b[39m vehicle_blueprint\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaypoints \u001b[38;5;241m=\u001b[39m waypoints\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactorlist \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mCarlaEnvironment.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mspawn_actor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle_blueprint, carla\u001b[38;5;241m.\u001b[39mTransform(carla\u001b[38;5;241m.\u001b[39mLocation(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m23.6\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m137.5\u001b[39m,z\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),carla\u001b[38;5;241m.\u001b[39mRotation(yaw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_waypoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m, in \u001b[0;36mCarlaEnvironment.get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     curr_vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mget_velocity()\n\u001b[1;32m     44\u001b[0m     curr_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mget_location()\n\u001b[0;32m---> 45\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_loc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaypoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     state\u001b[38;5;241m.\u001b[39mextend([curr_vel, curr_yaw \u001b[38;5;241m-\u001b[39m waypoint[\u001b[38;5;241m1\u001b[39m], curr_yaw \u001b[38;5;241m-\u001b[39m waypoint[\u001b[38;5;241m2\u001b[39m], distance ])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    Location.distance(Location, float)\ndid not match C++ signature:\n    distance(carla::geom::Location {lvalue}, carla::geom::Location location)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    client = carla.Client('localhost', 2000)\n",
    "    client.set_timeout(2.0)\n",
    "\n",
    "    world = client.get_world()\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_blueprint = blueprint_library.filter('vehicle.jeep.wrangler_rubicon')[0]\n",
    "    \n",
    "    waypoints = read_waypoints(\"waypoint_distance_data.csv\")\n",
    "    env = CarlaEnvironment(world, vehicle_blueprint, waypoints)  # Implement CarlaEnvironment with the required state and action spaces\n",
    "\n",
    "    state_dim = 40\n",
    "    action_dim = 3\n",
    "\n",
    "    train_ppo_maml(env, state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ec3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
