{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import learn2learn as l2l\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "import torch.distributions as td\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(x))\n",
    "        actions = self.l3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class CarlaEnvironment:\n",
    "    def __init__(self, world, vehicle_blueprint, waypoints):\n",
    "        self.world = world\n",
    "        self.vehicle_blueprint = vehicle_blueprint\n",
    "        self.waypoints = waypoints\n",
    "        self.actorlist = []\n",
    "        self.current_waypoint_list = []\n",
    "        self.vehicle = self.world.spawn_actor(self.vehicle_blueprint, carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0)))\n",
    "        \n",
    "    def reset(self):\n",
    "        try: self.vehicle.destroy()\n",
    "        except: pass\n",
    "        self.vehicle = self.world.spawn_actor(self.vehicle_blueprint, carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0)))\n",
    "        self.current_waypoint = 0\n",
    "        self.current_waypoint_list.append(self.current_waypoint)\n",
    "        self.moving = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.detach().numpy()\n",
    "        self.current_waypoint_list.append(self.current_waypoint)\n",
    "        #print(action)\n",
    "        # Apply actions: Throttle, Steering, Brake\n",
    "        if self.current_waypoint <10:\n",
    "            action = [1.0, 0.0, 0.0]\n",
    "        if self.current_waypoint ==10:\n",
    "            self.moving = True            \n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle = float(action[0]) , steer = float(action[1]) , brake = float(action[2])))\n",
    "       \n",
    "        #self.actorlist.append(self.vehicle)\n",
    "\n",
    "        # Get next state\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(next_state)\n",
    "       \n",
    "        # Check if the episode is done\n",
    "        done = self.is_done(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "#     def get_state(self):\n",
    "#         curr_vel = self.vehicle.get_velocity()\n",
    "#         curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "#         waypoint = self.waypoints[self.current_waypoint]\n",
    "#         return np.array([curr_vel, curr_yaw - waypoint[1], curr_yaw - waypoint[2], waypoint[3]])\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = []\n",
    "        curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "        \n",
    "        \n",
    "        for i in range(10):\n",
    "            if self.current_waypoint + i < len(self.waypoints):\n",
    "                waypoint = self.waypoints[self.current_waypoint + i]\n",
    "                curr_vel = self.vehicle.get_velocity()\n",
    "                curr_loc = self.vehicle.get_location()\n",
    "                vx = curr_vel.x\n",
    "                vy = curr_vel.y\n",
    "                v = ((vx)**2+(vy)**2)**0.5\n",
    "                if curr_loc.x==0 and curr_loc.y==0:\n",
    "                    curr_loc = carla.Location(x=waypoint[4], y=waypoint[5])\n",
    "                distance = curr_loc.distance(carla.Location(x=waypoint[4],y=waypoint[5]))\n",
    "                \n",
    "                state.extend([v, curr_yaw - waypoint[1], curr_yaw - waypoint[2], distance ])\n",
    "            else:\n",
    "                state.extend([0, 0, 0, 0])\n",
    "               \n",
    "        return np.array(state)\n",
    "\n",
    "    def generate_waypoints(self):\n",
    "        # Implement waypoint generation logic\n",
    "        pass\n",
    "\n",
    "    def calculate_reward(self, state):\n",
    "        reward = -(np.linalg.norm(state[0] - self.waypoints[self.current_waypoint][0]) - (np.linalg.norm (state[1])) - (np.linalg.norm ( state[2])) - (np.linalg.norm (state[3])) + self.current_waypoint)\n",
    "        return reward\n",
    "\n",
    "    def is_done(self, state):\n",
    "        curr_loc = self.vehicle.get_location()\n",
    "        if curr_loc.x==0 and curr_loc.y==0:\n",
    "            curr_loc = carla.Location(x=self.waypoints[self.current_waypoint][4],y=self.waypoints[self.current_waypoint][5])\n",
    " \n",
    "        distance = curr_loc.distance(carla.Location(x=self.waypoints[self.current_waypoint][4],y=self.waypoints[self.current_waypoint][5]))\n",
    "\n",
    "        if distance < 1:  # If the vehicle is close to the next waypoint\n",
    "            self.current_waypoint += 1\n",
    "\n",
    "        if self.current_waypoint >= len(self.waypoints):\n",
    "            return True\n",
    "    \n",
    "        \n",
    "        return False\n",
    "\n",
    "    def set_weather(self, weather):\n",
    "        self.world.set_weather(weather)\n",
    "        \n",
    "    def destroy(self):\n",
    "        for actor in self.actorlist:\n",
    "            actor.destroy()\n",
    "        print(\"All Cleared\")\n",
    "\n",
    "def set_conditions(env, condition):\n",
    "    \n",
    "        precipitation = condition[0]\n",
    "        precipitation_deposits = condition[1]\n",
    "        tire_friction = condition[2]\n",
    "        weather = carla.WeatherParameters( precipitation = precipitation,\n",
    "                                            precipitation_deposits = precipitation_deposits)\n",
    "        world.set_weather(weather)\n",
    "\n",
    "        tire_condition = carla.WheelPhysicsControl(tire_friction = tire_friction)\n",
    "\n",
    "        front_left_wheel  = carla.WheelPhysicsControl(tire_friction = tire_friction)\n",
    "        front_right_wheel = carla.WheelPhysicsControl(tire_friction = tire_friction)\n",
    "        rear_left_wheel   = carla.WheelPhysicsControl(tire_friction = tire_friction)\n",
    "        rear_right_wheel  = carla.WheelPhysicsControl(tire_friction = tire_friction)\n",
    "\n",
    "        wheels = [front_left_wheel, front_right_wheel, rear_left_wheel, rear_right_wheel]\n",
    "        \n",
    "        physics_control = env.vehicle.get_physics_control()\n",
    "        physics_control.wheels = wheels\n",
    "        env.vehicle.apply_physics_control(physics_control)\n",
    "\n",
    "def train_ppo_maml(env, state_dim, action_dim, lr=1e-4, n_iters=1000000, inner_loop_steps=5):\n",
    "    \n",
    "    p_low = 0\n",
    "    p_mid = 50\n",
    "    p_high = 100\n",
    "\n",
    "    pd_low = 50\n",
    "    pd_high = 100\n",
    "\n",
    "    tf_low = 0.5\n",
    "    tf_mid = 1.5\n",
    "    tf_high = 2.5\n",
    "    \n",
    "    conditions = [[p_low, pd_low, tf_high],\n",
    "            [p_low, pd_low, tf_mid],\n",
    "            [p_low, pd_high, tf_low],\n",
    "            [p_mid, pd_low, tf_low],\n",
    "            [p_mid, pd_high, tf_low],\n",
    "            [p_mid, pd_low, tf_mid],\n",
    "            [p_high, pd_low, tf_high],\n",
    "            [p_high, pd_high, tf_low],\n",
    "            [p_low, pd_high, tf_mid],\n",
    "            [p_high, pd_low, tf_mid]]\n",
    "    \n",
    "    n_tasks = len(conditions)\n",
    "\n",
    "    \n",
    "    \n",
    "    base_model = NeuralNetwork(state_dim, action_dim)\n",
    "    maml = l2l.algorithms.MAML(base_model, lr=lr)\n",
    "    optimizer = optim.Adam(maml.parameters(), lr=lr)\n",
    "   \n",
    "    iteration = 1\n",
    "    for i in range(n_iters):\n",
    "        condition = random.choice(conditions)\n",
    "        set_conditions(env,condition)\n",
    "    \n",
    "        trajectories, state_tensors, action_tensors, rewards = sample_trajectories(env, maml)\n",
    "        state_tensors_stacked = torch.stack(state_tensors)\n",
    "        action_tensors_stacked = torch.stack(action_tensors)\n",
    "        \n",
    "        for _ in range(inner_loop_steps):\n",
    "        \n",
    "            learner = maml.clone()\n",
    "            log_probs = compute_log_probs(learner, state_tensors_stacked, action_tensors_stacked)\n",
    "            loss = policy_loss(log_probs, rewards)\n",
    "            \n",
    "            # error = nn.MSELoss()(learner(state_tensors_stacked), action_tensors_stacked)\n",
    "\n",
    "            learner.adapt(loss)\n",
    "            \n",
    "            #error = nn.MSELoss()(learner(state_tensors_stacked), action_tensors_stacked)\n",
    "            log_probs = compute_log_probs(learner, state_tensors_stacked, action_tensors_stacked)\n",
    "            loss = policy_loss(log_probs, rewards)\n",
    "            #error.backward(retain_graph=True)\n",
    "\n",
    "        # Evaluating the model\n",
    "        p_evaluate = random.uniform(p_low, p_high)\n",
    "        pd_evaluate = random.uniform(pd_low, pd_high)\n",
    "        tf_evaluate = random.uniform(tf_low, tf_high)\n",
    "        evaluating_condition = [p_evaluate, pd_evaluate, tf_evaluate]\n",
    "        set_conditions(env, evaluating_condition)\n",
    "        \n",
    "        trajectories, state_tensors, action_tensors, rewards = sample_trajectories(env, maml)\n",
    "        state_tensors_stacked = torch.stack(state_tensors)\n",
    "        action_tensors_stacked = torch.stack(action_tensors)\n",
    "\n",
    "        log_probs = compute_log_probs(learner, state_tensors_stacked, action_tensors_stacked)\n",
    "        loss = policy_loss(log_probs, rewards)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # error = nn.MSELoss()(learner(state_tensors_stacked), action_tensors_stacked)\n",
    "        # error.backward(retain_graph=True)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        print(f\"Iteration: \", iteration)\n",
    "\n",
    "        #if iteration % 1000 == 0:\n",
    "        #    print(\"Iteration for {condition}: \", iteration)\n",
    "\n",
    "def sample_weather_conditions(num_weather_conditions):\n",
    "    return carla.WeatherParameters.HardRainNoon\n",
    "\n",
    "def compute_log_probs(policy, states, actions):\n",
    "    # states = torch.stack(states)\n",
    "    # actions = torch.stack(actions)\n",
    "    action_logits = policy(states)\n",
    "    action_probs = F.softmax(action_logits,  dim=1)\n",
    "    action_indices = actions.argmax(dim=1)\n",
    "\n",
    "    log_probs = torch.log(action_probs[torch.arange(len(action_indices)), action_indices])\n",
    "    \n",
    "    return log_probs\n",
    "\n",
    "    # std = log_std.exp()\n",
    "    # normal_dist = td.Normal(mu, std)\n",
    "    # log_probs = normal_dist.log_prob(actions).sum(dim=-1)\n",
    "    return log_probs\n",
    "\n",
    "def policy_loss(log_probs, rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        cumulative_reward = reward + gamma * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    loss = -torch.mean(log_probs * discounted_rewards)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sample_trajectories(env, model, num_episodes=10, max_episode_length=1000):\n",
    "#     model.to(\"cpu\")\n",
    "    trajectories = []\n",
    "    traj_states = []\n",
    "    traj_action = []\n",
    "    state_tensors = []\n",
    "    action_tensors = []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        \n",
    "        state = env.reset()              \n",
    "        trajectory = []\n",
    "        print(\"trajectories: \",len(trajectories))\n",
    "        while True:\n",
    "            \n",
    "#             state_tensor = torch.tensor(state).to(torch.float32)\n",
    "            state_tensor = torch.from_numpy(state.astype(np.float32))\n",
    "            state_tensors.append(state_tensor)\n",
    "            action_tensor = model(state_tensor)\n",
    "#             action_distribution = Normal(action_mean, torch.tensor(0.1))\n",
    "#             action = action_distribution.sample().detach().numpy().astype(float)\n",
    "            \n",
    "            action = action_tensor\n",
    "#             action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "            action_tensors.append(action)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            waypoint_copy = env.current_waypoint\n",
    "            # if not j%200 : print(\"Reward: \", reward, )\n",
    "\n",
    "            if len(env.current_waypoint_list) >=51 and env.current_waypoint >10:\n",
    "                if env.current_waypoint_list[len(env.current_waypoint_list)-1] == env.current_waypoint_list[len(env.current_waypoint_list) - 50] and env.moving:\n",
    "                    done = True\n",
    "            #state_tensors = torch.tensor(state_tensor, dtype=torch.float32)\n",
    "            if done or (env.current_waypoint == len(env.waypoints) - 1):\n",
    "                #env.vehicle.destroy()\n",
    "                break  \n",
    "\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories, state_tensors, action_tensors, rewards\n",
    "\n",
    "            \n",
    "def read_waypoints(file_path):\n",
    "    waypoints = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            v, yaw_c, yaw_n, r, x, y = map(float, row)\n",
    "            waypoints.append((v, yaw_c, yaw_n, r, x, y))\n",
    "            \n",
    "    return waypoints                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API \n",
      "WARNING: Client API version     = 0.9.14 \n",
      "WARNING: Simulator API version  = 0.9.14-4-gf14acb257-dirty \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories:  0\n",
      "trajectories:  1\n",
      "trajectories:  2\n",
      "trajectories:  3\n",
      "trajectories:  4\n",
      "trajectories:  5\n",
      "trajectories:  6\n",
      "trajectories:  7\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    client = carla.Client('localhost', 2000)\n",
    "    client.set_timeout(2.0)\n",
    "\n",
    "    world = client.get_world()\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_blueprint = blueprint_library.filter(\"wrangler_rubicon\")[0]\n",
    "\n",
    "    waypoints = read_waypoints(\"waypoint_with_xy.csv\")\n",
    "    env = CarlaEnvironment(world, vehicle_blueprint, waypoints)  # Implement CarlaEnvironment with the required state and action spaces\n",
    "\n",
    "    state_dim = 40\n",
    "    action_dim = 3\n",
    "\n",
    "    train_ppo_maml(env, state_dim, action_dim)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
