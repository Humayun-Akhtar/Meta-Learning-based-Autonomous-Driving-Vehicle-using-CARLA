{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918ddc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import learn2learn as l2l\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(x))\n",
    "        #action_logits = self.l3(x)\n",
    "        actions = self.l3(x)\n",
    "        #ction_probs = F.softmax(action_logits, dim=1)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class CarlaEnvironment:\n",
    "    def __init__(self, world, vehicle_blueprint, waypoints):\n",
    "        self.world = world\n",
    "        self.vehicle_blueprint = vehicle_blueprint\n",
    "        self.waypoints = waypoints\n",
    "        self.actorlist = []\n",
    "        # self.vehicle = self.world.spawn_actor(self.vehicle_blueprint, carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0)))\n",
    "        \n",
    "    def reset(self):\n",
    "        self.vehicle = self.world.spawn_actor(self.vehicle_blueprint, carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0)))\n",
    "        self.current_waypoint = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.detach().numpy()\n",
    "        #print(action)\n",
    "        # Apply actions: Throttle, Steering, Brake\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle = float(action[0]) , steer = float(action[1]) , brake = float(action[2])))\n",
    "       \n",
    "        #self.actorlist.append(self.vehicle)\n",
    "\n",
    "        # Get next state\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.calculate_reward(next_state)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.is_done(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "#     def get_state(self):\n",
    "#         curr_vel = self.vehicle.get_velocity()\n",
    "#         curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "#         waypoint = self.waypoints[self.current_waypoint]\n",
    "#         return np.array([curr_vel, curr_yaw - waypoint[1], curr_yaw - waypoint[2], waypoint[3]])\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = []\n",
    "        curr_yaw = self.vehicle.get_transform().rotation.yaw\n",
    "        \n",
    "        \n",
    "        for i in range(10):\n",
    "            if self.current_waypoint + i < len(self.waypoints):\n",
    "                waypoint = self.waypoints[self.current_waypoint + i]\n",
    "                curr_vel = self.vehicle.get_velocity()\n",
    "                curr_loc = self.vehicle.get_location()\n",
    "                vx = curr_vel.x\n",
    "                vy = curr_vel.y\n",
    "                v = ((vx)**2+(vy)**2)**0.5\n",
    "                distance = curr_loc.distance(carla.Location(x=waypoint[4],y=waypoint[5]))\n",
    "                state.extend([v, curr_yaw - waypoint[1], curr_yaw - waypoint[2], distance ])\n",
    "            else:\n",
    "                state.extend([0, 0, 0, 0])\n",
    "        return np.array(state)\n",
    "\n",
    "    def generate_waypoints(self):\n",
    "        # Implement waypoint generation logic\n",
    "        pass\n",
    "\n",
    "    def calculate_reward(self, state):\n",
    "        reward = -(np.linalg.norm(state[0] - self.waypoints[self.current_waypoint][0]) - (np.linalg.norm (state[1])) - (np.linalg.norm ( state[2])) - (np.linalg.norm (state[3])) + self.current_waypoint)\n",
    "        return reward\n",
    "\n",
    "    def is_done(self, state):\n",
    "        if np.abs(state[3]) < 0.20:  # If the vehicle is close to the next waypoint\n",
    "            self.current_waypoint += 1\n",
    "\n",
    "        if self.current_waypoint >= len(self.waypoints):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def set_weather(self, weather):\n",
    "        self.world.set_weather(weather)\n",
    "        \n",
    "    def destroy(self):\n",
    "        for actor in self.actorlist:\n",
    "            actor.destroy()\n",
    "        print(\"All Cleared\")\n",
    "\n",
    "\n",
    "def train_ppo_maml(env, state_dim, action_dim, lr=1e-4, n_iters=1000000, inner_loop_steps=5):\n",
    "    base_model = NeuralNetwork(state_dim, action_dim)\n",
    "    maml = l2l.algorithms.MAML(base_model, lr=lr)\n",
    "    optimizer = optim.Adam(maml.parameters(), lr=lr)\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        weather = carla.WeatherParameters.HardRainNoon\n",
    "        #for weather in weather_conditions:\n",
    "        env.set_weather(weather)\n",
    "        trajectories, state_tensors, action_tensors = sample_trajectories(env, maml)\n",
    "        state_tensors_stacked = torch.stack(state_tensors)\n",
    "        action_tensors_stacked = torch.stack(action_tensors)\n",
    "        for _ in range(inner_loop_steps):\n",
    "            clone = maml.clone()\n",
    "            error = nn.MSELoss()(clone(state_tensors_stacked), action_tensors_stacked)\n",
    "            clone.adapt(error)\n",
    "            error = nn.MSELoss()(clone(state_tensors_stacked), action_tensors_stacked)\n",
    "            error.backward(retain_graph=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"Iteration: \", iteration)\n",
    "\n",
    "\n",
    "def sample_weather_conditions(num_weather_conditions):\n",
    "    return carla.WeatherParameters.HardRainNoon\n",
    "\n",
    "def sample_trajectories(env, model, num_episodes=10, max_episode_length=1000):\n",
    "#     model.to(\"cpu\")\n",
    "    trajectories = []\n",
    "    traj_states = []\n",
    "    traj_action = []\n",
    "    state_tensors = []\n",
    "    action_tensors = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()                \n",
    "        trajectory = []\n",
    "\n",
    "        for j in range(max_episode_length):\n",
    "            \n",
    "#             state_tensor = torch.tensor(state).to(torch.float32)\n",
    "            state_tensor = torch.from_numpy(state.astype(np.float32))\n",
    "            state_tensors.append(state_tensor)\n",
    "            action_tensor = model(state_tensor)\n",
    "#             action_distribution = Normal(action_mean, torch.tensor(0.1))\n",
    "#             action = action_distribution.sample().detach().numpy().astype(float)\n",
    "            \n",
    "            action = action_tensor\n",
    "#             action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "            action_tensors.append(action)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            if not j%200 : print(\"Reward: \", reward, '  J: ', j )\n",
    "            #state_tensors = torch.tensor(state_tensor, dtype=torch.float32)\n",
    "            if done or j==max_episode_length-1:\n",
    "                env.vehicle.destroy()\n",
    "                break  \n",
    "\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories, state_tensors, action_tensors\n",
    "\n",
    "            \n",
    "def read_waypoints(file_path):\n",
    "    waypoints = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            v, yaw_c, yaw_n, r, x, y = map(float, row)\n",
    "            waypoints.append((v, yaw_c, yaw_n, r, x, y))\n",
    "            \n",
    "    return waypoints                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f59e69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API \n",
      "WARNING: Client API version     = 0.9.14 \n",
      "WARNING: Simulator API version  = 0.9.14-4-gf14acb257 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7872778177261353   J:  200\n",
      "Reward:  1.7872778177261353   J:  400\n",
      "Reward:  1.7872778177261353   J:  600\n",
      "Reward:  1.6391003727912903   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7457160949707031   J:  200\n",
      "Reward:  1.6628984808921814   J:  400\n",
      "Reward:  1.5505987405776978   J:  600\n",
      "Reward:  1.4037348628044128   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7720582485198975   J:  200\n",
      "Reward:  1.712706446647644   J:  400\n",
      "Reward:  1.613811194896698   J:  600\n",
      "Reward:  1.4777296781539917   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.75984787940979   J:  200\n",
      "Reward:  1.692269742488861   J:  400\n",
      "Reward:  1.616624116897583   J:  600\n",
      "Reward:  1.616624116897583   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7845237255096436   J:  200\n",
      "Reward:  1.7845237255096436   J:  400\n",
      "Reward:  1.7845237255096436   J:  600\n",
      "Reward:  1.7845237255096436   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7589322328567505   J:  200\n",
      "Reward:  1.6846906542778015   J:  400\n",
      "Reward:  1.574946403503418   J:  600\n",
      "Reward:  1.4365909099578857   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7609350681304932   J:  200\n",
      "Reward:  1.6920958757400513   J:  400\n",
      "Reward:  1.5888946056365967   J:  600\n",
      "Reward:  1.4521267414093018   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7733991146087646   J:  200\n",
      "Reward:  1.7146271467208862   J:  400\n",
      "Reward:  1.6448404788970947   J:  600\n",
      "Reward:  1.518943428993225   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.716515302658081   J:  200\n",
      "Reward:  1.625649869441986   J:  400\n",
      "Reward:  1.4971483945846558   J:  600\n",
      "Reward:  1.3811781406402588   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7599375247955322   J:  200\n",
      "Reward:  1.7107194662094116   J:  400\n",
      "Reward:  1.6072964072227478   J:  600\n",
      "Reward:  1.4739283919334412   J:  800\n",
      "Iteration:  0\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.685568928718567   J:  200\n",
      "Reward:  1.685568928718567   J:  400\n",
      "Reward:  1.4122207164764404   J:  600\n",
      "Reward:  1.3221073746681213   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7926090955734253   J:  200\n",
      "Reward:  1.7926090955734253   J:  400\n",
      "Reward:  1.7926090955734253   J:  600\n",
      "Reward:  1.7926090955734253   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.747741937637329   J:  200\n",
      "Reward:  1.5194012522697449   J:  400\n",
      "Reward:  1.4350529909133911   J:  600\n",
      "Reward:  1.4350529909133911   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7923439741134644   J:  200\n",
      "Reward:  1.726288914680481   J:  400\n",
      "Reward:  1.726288914680481   J:  600\n",
      "Reward:  1.726288914680481   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7891571521759033   J:  200\n",
      "Reward:  1.7891571521759033   J:  400\n",
      "Reward:  1.7891571521759033   J:  600\n",
      "Reward:  1.7891571521759033   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7926710844039917   J:  200\n",
      "Reward:  1.7926710844039917   J:  400\n",
      "Reward:  1.7926710844039917   J:  600\n",
      "Reward:  1.7926710844039917   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.755655288696289   J:  200\n",
      "Reward:  1.755655288696289   J:  400\n",
      "Reward:  1.755655288696289   J:  600\n",
      "Reward:  1.755655288696289   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7612836360931396   J:  200\n",
      "Reward:  1.6899812817573547   J:  400\n",
      "Reward:  1.5830646753311157   J:  600\n",
      "Reward:  1.5830646753311157   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7709789276123047   J:  200\n",
      "Reward:  1.7709789276123047   J:  400\n",
      "Reward:  1.7709789276123047   J:  600\n",
      "Reward:  1.7709789276123047   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.770947813987732   J:  200\n",
      "Reward:  1.6889135241508484   J:  400\n",
      "Reward:  1.5762913227081299   J:  600\n",
      "Reward:  1.3995742797851562   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7611792087554932   J:  200\n",
      "Reward:  1.6905055046081543   J:  400\n",
      "Reward:  1.5839386582374573   J:  600\n",
      "Reward:  1.5520984530448914   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.772019386291504   J:  200\n",
      "Reward:  1.6907317638397217   J:  400\n",
      "Reward:  1.5846818685531616   J:  600\n",
      "Reward:  1.5496627688407898   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.746694564819336   J:  200\n",
      "Reward:  1.6405117511749268   J:  400\n",
      "Reward:  1.5490331649780273   J:  600\n",
      "Reward:  1.4041910767555237   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.770242691040039   J:  200\n",
      "Reward:  1.7078863382339478   J:  400\n",
      "Reward:  1.63291734457016   J:  600\n",
      "Reward:  1.5748974084854126   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.771364450454712   J:  200\n",
      "Reward:  1.7081291675567627   J:  400\n",
      "Reward:  1.6397884488105774   J:  600\n",
      "Reward:  1.614940106868744   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.759063959121704   J:  200\n",
      "Reward:  1.7099403142929077   J:  400\n",
      "Reward:  1.6101974248886108   J:  600\n",
      "Reward:  1.5144026279449463   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7725151777267456   J:  200\n",
      "Reward:  1.6896429657936096   J:  400\n",
      "Reward:  1.5757502317428589   J:  600\n",
      "Reward:  1.5451207160949707   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7924598455429077   J:  200\n",
      "Reward:  1.7456095218658447   J:  400\n",
      "Reward:  1.6601406931877136   J:  600\n",
      "Reward:  1.541549801826477   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7352887392044067   J:  200\n",
      "Reward:  1.6311626434326172   J:  400\n",
      "Reward:  1.511595368385315   J:  600\n",
      "Reward:  1.3290688395500183   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7923520803451538   J:  200\n",
      "Reward:  1.7923520803451538   J:  400\n",
      "Reward:  1.7923520803451538   J:  600\n",
      "Reward:  1.710564136505127   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.606202781200409   J:  200\n",
      "Reward:  1.606202781200409   J:  400\n",
      "Reward:  1.3668565154075623   J:  600\n",
      "Reward:  1.3668565154075623   J:  800\n",
      "Reward:  139.78680419921875   J:  0\n",
      "Reward:  1.7875052690505981   J:  200\n",
      "Reward:  1.7875052690505981   J:  400\n",
      "Reward:  1.6765803694725037   J:  600\n",
      "Reward:  1.6765803694725037   J:  800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: failed to destroy actor 1324 : time-out of 2000ms while waiting for the simulator, make sure the simulator is ready and connected to localhost:2000 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "time-out of 2000ms while waiting for the simulator, make sure the simulator is ready and connected to localhost:2000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[1;32m     14\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_ppo_maml\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m, in \u001b[0;36mtrain_ppo_maml\u001b[0;34m(env, state_dim, action_dim, lr, n_iters, inner_loop_steps)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#for weather in weather_conditions:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m env\u001b[38;5;241m.\u001b[39mset_weather(weather)\n\u001b[0;32m--> 125\u001b[0m trajectories, state_tensors, action_tensors \u001b[38;5;241m=\u001b[39m \u001b[43msample_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m state_tensors_stacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(state_tensors)\n\u001b[1;32m    127\u001b[0m action_tensors_stacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(action_tensors)\n",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m, in \u001b[0;36msample_trajectories\u001b[0;34m(env, model, num_episodes, max_episode_length)\u001b[0m\n\u001b[1;32m    151\u001b[0m     action_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m--> 153\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                \n\u001b[1;32m    154\u001b[0m         trajectory \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_episode_length):\n\u001b[1;32m    157\u001b[0m             \n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m#             state_tensor = torch.tensor(state).to(torch.float32)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m, in \u001b[0;36mCarlaEnvironment.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicle_blueprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m23.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m137.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcarla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_waypoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: time-out of 2000ms while waiting for the simulator, make sure the simulator is ready and connected to localhost:2000"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    client = carla.Client('localhost', 2000)\n",
    "    client.set_timeout(2.0)\n",
    "\n",
    "    world = client.get_world()\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_blueprint = blueprint_library.filter(\"wrangler_rubicon\")[0]\n",
    "\n",
    "    waypoints = read_waypoints(\"waypoint_with_xy.csv\")\n",
    "    env = CarlaEnvironment(world, vehicle_blueprint, waypoints)  # Implement CarlaEnvironment with the required state and action spaces\n",
    "\n",
    "    state_dim = 40\n",
    "    action_dim = 3\n",
    "\n",
    "    train_ppo_maml(env, state_dim, action_dim)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd211f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e72e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049ad942",
   "metadata": {},
   "source": [
    "In the example code you provided, X represents the input features (or states in the case of a reinforcement learning problem), and Y represents the target values (or true action values in a reinforcement learning problem) for training the model.\n",
    "\n",
    "The code snippet demonstrates how to use Learn2Learn's MAML implementation for a simple linear regression problem with 20 input features and 10 output targets. Here's a brief explanation of each line of code:\n",
    "\n",
    "    linear = l2l.algorithms.MAML(nn.Linear(20, 10), lr=0.01): Create a MAML instance with a simple linear model and a learning rate of 0.01.\n",
    "    clone = linear.clone(): Clone the base model to create a temporary model for adaptation.\n",
    "    error = loss(clone(X), y): Compute the loss (error) between the model's predictions on the input X and the true target values Y.\n",
    "    clone.adapt(error): Update the temporary model's parameters using the computed loss (one step of gradient descent).\n",
    "    error = loss(clone(X), y): Compute the loss again after adaptation.\n",
    "    error.backward(): Perform backpropagation to compute gradients for updating the base model.\n",
    "\n",
    "In the context of your problem, X would correspond to the state values (with 40 elements, considering the current waypoint and the upcoming 9 waypoints), and Y would be the true action values to follow the given waypoints closely.\n",
    "\n",
    "Keep in mind that this example is for a simple regression problem, and you would need to adapt it to suit the reinforcement learning setting, using the appropriate loss function and input-output pairs (states and actions) from the sampled trajectories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
