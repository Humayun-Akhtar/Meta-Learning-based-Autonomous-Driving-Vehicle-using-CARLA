{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import carla\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import learn2learn as l2l\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "class Vector:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "threshold = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "CSV_FILE = \"waypoint.csv\"\n",
    "EPISODES = 500\n",
    "STEPS = 50000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "    \n",
    "def waypoint_based_action(vehicle, next_waypoint):\n",
    "    current_transform = vehicle.get_transform()\n",
    "    current_yaw = current_transform.rotation.yaw\n",
    "\n",
    "    next_location = carla.Location(x=next_waypoint[0], y=next_waypoint[1])\n",
    "    desired_yaw = math.degrees(math.atan2(next_location.y - current_transform.location.y, \n",
    "                                          next_location.x - current_transform.location.x))\n",
    "    yaw_diff = (desired_yaw - current_yaw) % 360\n",
    "\n",
    "    if yaw_diff > 180:\n",
    "        yaw_diff -= 360\n",
    "\n",
    "    if yaw_diff > 5:\n",
    "        action = 3  # Steer right\n",
    "    elif yaw_diff < -5:\n",
    "        action = 2  # Steer left\n",
    "    else:\n",
    "        action = 0  # Go straight\n",
    "#     print(f'action decided: {action}, current yaw: {yaw_diff}')\n",
    "    return torch.tensor([[action]], dtype=torch.long)\n",
    "\n",
    "    \n",
    "\n",
    "def read_waypoints(file_path):\n",
    "    waypoints = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            x, y, yaw = map(float, row)\n",
    "            waypoints.append((x, y, yaw))\n",
    "            \n",
    "    return waypoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CarlaEnv(gym.Env):\n",
    "    def __init__(self, vehicle, waypoints):\n",
    "        super(CarlaEnv, self).__init__()\n",
    "        self.vehicle = vehicle\n",
    "        self.waypoints = waypoints\n",
    "        self.current_waypoint_index = 0\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # [throttle, brake, steer_left, steer_right]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "\n",
    "        self.lane_invasion = False\n",
    "#         print('CarlaEnv made')\n",
    "#         self.lane_invasion_sensor = self._spawn_lane_invasion_sensor()\n",
    "        \n",
    "#     def _spawn_lane_invasion_sensor(self):\n",
    "#         sensor_bp = self.world.get_blueprint_library().find('sensor.other.lane_invasion')\n",
    "#         sensor_transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "#         sensor = self.world.spawn_actor(sensor_bp, sensor_transform, attach_to=self.vehicle)\n",
    "#         sensor.listen(lambda event: self._on_lane_invasion(event))\n",
    "#         return sensor\n",
    "\n",
    "    def _on_lane_invasion(self, event):\n",
    "#         print(\"lane invasion detected\")\n",
    "        self.lane_invasion = True    \n",
    "\n",
    "    def reset(self):\n",
    "        vel = carla.Vector3D()\n",
    "        vel.x = 0\n",
    "        vel.y = 0\n",
    "        vel.z = 0\n",
    "        self.vehicle.set_target_velocity(vel)\n",
    "        x, y, yaw = self.waypoints[0]\n",
    "        transform = carla.Transform(carla.Location(x, y), carla.Rotation(yaw=yaw))\n",
    "        self.vehicle.set_transform(transform)\n",
    "#         print(\"reset done\")\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        throttle = 0\n",
    "        brake = 0\n",
    "        steer = 0\n",
    "\n",
    "        if action == 0:\n",
    "            throttle = 1.0\n",
    "        elif action == 1:\n",
    "            brake = 1.0\n",
    "        elif action == 2:\n",
    "            steer = -1.0\n",
    "        \n",
    "        elif action == 3:\n",
    "            steer = 1.0\n",
    "            \n",
    "\n",
    "        control = carla.VehicleControl(throttle=throttle, brake=brake, steer=steer)\n",
    "        self.vehicle.apply_control(control)\n",
    "\n",
    "        next_waypoint = self.waypoints[self.current_waypoint_index]\n",
    "        next_waypoint_location = carla.Location(x=next_waypoint[0], y=next_waypoint[1])\n",
    "        current_location = self.vehicle.get_location()\n",
    "        distance = current_location.distance(next_waypoint_location)\n",
    "\n",
    "        if distance < threshold:\n",
    "            self.current_waypoint_index += 1\n",
    "            if self.current_waypoint_index >= len(self.waypoints):\n",
    "                self.current_waypoint_index = 0\n",
    "\n",
    "        reward = self._get_reward(distance, control)\n",
    "\n",
    "        done = False\n",
    "        if self.current_waypoint_index == len(self.waypoints) - 1:\n",
    "            done = True\n",
    "#         print (f'step taken: Throttle: {throttle} and steer: {steer}')\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        location = self.vehicle.get_location()\n",
    "        orientation = self.vehicle.get_transform().rotation.yaw\n",
    "        speed = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(speed.x**2 + speed.y**2 + speed.z**2)\n",
    "        next_waypoint = self.waypoints[self.current_waypoint_index]\n",
    "        next_waypoint_location = carla.Location(x=next_waypoint[0], y=next_waypoint[1])\n",
    "        distance = location.distance(next_waypoint_location)\n",
    "#         print('observation taken')\n",
    "        return np.array([location.x, location.y, orientation, speed, distance])\n",
    "\n",
    "\n",
    "    def _get_reward(self, distance, control):\n",
    "        reward = 0\n",
    "        if control.throttle > 0:\n",
    "            reward += 0\n",
    "        if distance < threshold:\n",
    "            reward += 10    \n",
    "        # Apply a large negative reward for lane invasion\n",
    "        if self.lane_invasion:\n",
    "            reward -= 1000  # Adjust the value as needed\n",
    "            self.lane_invasion = True    \n",
    "#         print('reward calculated')    \n",
    "        return reward\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(CarlaEnv):\n",
    "    def __init__(self, episodes, steps, sample_size, learner, meta_optim):\n",
    "        self.episodes = episodes\n",
    "        self.steps = steps\n",
    "        self.sample_size = sample_size\n",
    "        self.learner = learner\n",
    "        self.meta_optim = meta_optim\n",
    "        \n",
    "\n",
    "    def setup(self, weather):\n",
    "        \n",
    "        try:\n",
    "            state_size = 5\n",
    "            action_size = 4\n",
    "            agent = PolicyNetwork(state_size, action_size)\n",
    "            #target_agent = copy.deepcopy(agent)\n",
    "            #optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "            memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "\n",
    "            # Read waypoints from CSV\n",
    "            waypoints = read_waypoints(CSV_FILE)\n",
    "\n",
    "            # Connect to CARLA server\n",
    "            client = carla.Client('localhost', 2000)\n",
    "            client.set_timeout(10.0)\n",
    "\n",
    "            # Load CARLA world\n",
    "            world = client.get_world()\n",
    "            \n",
    "            current_weather = weather\n",
    "            print(current_weather)\n",
    "            if current_weather == \"HardRainNoon\":\n",
    "                world.set_weather(carla.WeatherParameters.HardRainNoon)\n",
    "\n",
    "            \n",
    "            blueprint_library = world.get_blueprint_library()\n",
    "\n",
    "            # Spawn vehicle\n",
    "            vehicle_bp = random.choice(blueprint_library.filter('wrangler_rubicon'))\n",
    "            spawn_point = carla.Transform(carla.Location(x=-23.6,y=137.5,z=1),carla.Rotation(yaw=0))\n",
    "            self.vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "\n",
    "            sensor_bp = world.get_blueprint_library().find('sensor.other.lane_invasion')\n",
    "            sensor_transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "            self.sensor = world.spawn_actor(sensor_bp, sensor_transform, attach_to=self.vehicle)\n",
    "            self.sensor.listen(lambda event: self.env._on_lane_invasion(event))\n",
    "\n",
    "            # Create CARLA environment\n",
    "            self.env = CarlaEnv(self.vehicle, waypoints)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    def generate_traj(self):\n",
    "\n",
    "        self.states_buffer = np.empty((self.sample_size, self.steps))\n",
    "        action_buffer = np.empty((self.sample_size, self.steps))\n",
    "        rewards_buffer = np.empty((self.sample_size, self.steps))\n",
    "        next_states_buffer = np.empty((self.sample_size, self.steps))\n",
    "    \n",
    "        for episode in range(self.sample_size):\n",
    "            state = self.env.reset()\n",
    "            self.env.current_waypoint_index = 0\n",
    "            self.env.lane_invasion = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            next_states = []\n",
    "\n",
    "            for step in range(self.steps):\n",
    "                \n",
    "                #states = []\n",
    "                #actions = []\n",
    "                #rewards = []\n",
    "                #next_states = []\n",
    "\n",
    "                # Observe\n",
    "                state = self.env._get_observation()\n",
    "                next_waypoint = self.env.waypoints[self.env.current_waypoint_index]\n",
    "                action = waypoint_based_action(self.vehicle, next_waypoint)\n",
    "\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "\n",
    "            self.states_buffer.append(states)\n",
    "            action_buffer = np.stack((action_buffer, actions))\n",
    "            rewards_buffer = np.vstack((rewards_buffer, rewards))\n",
    "            next_states_buffer = np.vstack((next_states_buffer, next_state))\n",
    "\n",
    "            \n",
    "            #print(episode)\n",
    "        print(actions)\n",
    "\n",
    "    def destroy(self):\n",
    "        # Destroy the vehicle in CARLA\n",
    "        self.sensor.destroy()                      \n",
    "        self.vehicle.destroy()\n",
    "        print(\"All Cleared\")\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    def meta_train(self):\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(self.learner(self.states_buffer), action)\n",
    "\n",
    "        # Compute gradients and update model\n",
    "        self.learner.adapt(loss.item())\n",
    "\n",
    "        # Meta update\n",
    "        self.meta_optim.zero_grad()\n",
    "        mean_task_loss = torch.mean(torch.tensor(task_losses))\n",
    "        mean_task_loss.backward()\n",
    "        self.meta_optim.step()\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Version mismatch detected: You are trying to connect to a simulator that might be incompatible with this API \n",
      "WARNING: Client API version     = 0.9.14 \n",
      "WARNING: Simulator API version  = 0.9.14-4-gf14acb257-dirty \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HardRainNoon\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m train_obj \u001b[39m=\u001b[39m trainer(episodes, \u001b[39m100\u001b[39m, \u001b[39m50\u001b[39m, learner, meta_optim)\n\u001b[1;32m     20\u001b[0m train_obj\u001b[39m.\u001b[39msetup(\u001b[39m\"\u001b[39m\u001b[39mHardRainNoon\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m train_obj\u001b[39m.\u001b[39;49mgenerate_traj()\n\u001b[1;32m     24\u001b[0m train_obj\u001b[39m.\u001b[39mdestroy()\n",
      "Cell \u001b[0;32mIn[19], line 97\u001b[0m, in \u001b[0;36mtrainer.generate_traj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     95\u001b[0m     next_states\u001b[39m.\u001b[39mappend(next_state)\n\u001b[0;32m---> 97\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstates_buffer\u001b[39m.\u001b[39;49mappend(states)\n\u001b[1;32m     98\u001b[0m action_buffer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack((action_buffer, actions))\n\u001b[1;32m     99\u001b[0m rewards_buffer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((rewards_buffer, rewards))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "\n",
    "state_size = 5\n",
    "action_size = 4\n",
    "meta_lr = 0.001\n",
    "fast_lr = 0.01\n",
    "\n",
    "# Defining MAML model\n",
    "model = l2l.algorithms.MAML(PolicyNetwork(state_size, action_size), lr = fast_lr)\n",
    "\n",
    "# Defining meta-optimizer\n",
    "meta_optim = torch.optim.Adam(model.parameters(), lr = meta_lr)\n",
    "\n",
    "\n",
    "\n",
    "#train_obj.setup(\"HardRainNoon\")\n",
    "\n",
    "learner = model.clone()\n",
    "episodes = 100\n",
    "\n",
    "train_obj = trainer(episodes, 100, 50, learner, meta_optim)\n",
    "train_obj.setup(\"HardRainNoon\")\n",
    "train_obj.generate_traj()\n",
    "\n",
    "\n",
    "train_obj.destroy()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
